# Embedding & Context Review

## Executive Summary
- Retrieval uses MongoDB Atlas vector search with Redis caching but key mismatches along the pipeline block relevance.
- The ingestion path uploads raw embeddings to clients and lacks guardrails for large or repeated uploads.
- Prompt construction drops high-signal fragments and injects noisy placeholders, weakening grounding for the LLM.

## Issue Catalogue
| Severity | Area | Issue | Recommendation |
| --- | --- | --- | --- |
| Critical | Ingestion vs. Retrieval | Stored embeddings use `text-embedding-ada-002` while live queries use `text-embedding-3-small`, so vectors live in different spaces and semantic match collapses. | Standardise on a single embedding model for both flows and re-embed legacy chunks before re-enabling search. |
| High | API Surface | `/context/upload-file` returns full embedding arrays to callers, leaking proprietary vectors and ballooning payloads. | Return only summary metadata (chunk count, ids) and hide raw embeddings behind an internal diagnostic route. |
| High | Ingestion Robustness | Large uploads send every chunk in one OpenAI call; hitting service limits or a single 429 aborts the whole batch. | Batch the chunk list (e.g. 100 items/request) with retry/backoff and partial upsert logging; persist progress so operators can resume. |
| Medium | Retrieval | Hard cutoff `minScore = 0.75` often filters out valid Atlas results, leaving chat without context. | Lower/parameterise the threshold and fall back to top-k when nothing clears the bar; log the distribution for tuning. |
| Medium | Retrieval Failure Handling | Vector search exceptions bubble up and the chat handler lacks a fallback path. | Wrap `retrieveRelevantChunks` in a try/catch that degrades to cached history or keyword search so the user still gets a response. |
| Medium | Prompt Quality | Context shorter than 20 characters is discarded and, when empty, a literal instruction string is injected. | Keep high-signal short snippets (e.g. FAQ answers) and replace the placeholder with a system message flag instead of pseudo-context. |
| Medium | Knowledge Freshness | Results are cached for 120s even after new chunks are added; there is no cache busting on ingestion. | Push an explicit invalidation (Redis `del` by chatbot) after ingest jobs to ensure new knowledge is queryable instantly. |
| Low | Data Hygiene | Re-uploading the same file creates duplicate chunks with identical content. | Hash content per chatbot and skip inserts when a duplicate chunk already exists. |

## Detailed Notes

### 1. Embedding model mismatch (Critical)
- Evidence: ingestion relies on `text-embedding-ada-002` (`services/embeddingService.js:13`), while query-time embedding calls `text-embedding-3-small` (`lib/embed.js:20`).
- Impact: cosine similarity between vectors generated by different models is meaningless; Atlas search effectively retrieves random documents once both corpora exist.
- Action: pick one model (likely `text-embedding-3-small` for cost/perf) and migrate ingestion plus live services. Rebuild existing embeddings before lifting traffic.

### 2. Embedding leak in upload response (High)
- Evidence: `uploadContextFile` responds with `data: results` (`controllers/contextController.js:25-32`), where each document includes the full 1,536-d vector.
- Impact: exposes proprietary embeddings to anyone with panel access and sends multi-MB payloads for big uploads, causing timeouts.
- Action: trim the response to counts + identifiers, and log details server-side for audits.

### 3. Batch ingestion fragility (High)
- Evidence: `storeContextChunks` forwards the entire chunk list into a single `batchGenerateEmbeddings` call (`services/contextService.js:18`, `services/embeddingService.js:48`).
- Impact: any OpenAI throttling or 413 error aborts the whole upload, and there is no checkpointing—operators must restart from scratch.
- Action: chunk the embed calls (e.g. 100 inputs), add retry/backoff stratified by status, and record successes vs failures so resumable jobs are possible.

### 4. Score filtering too aggressive (Medium)
- Evidence: `retrieveRelevantChunks` drops results when `score < 0.75` (`services/queryService.js:9-24`). Atlas cosine scores routinely sit in 0.5–0.7 for valid matches.
- Impact: legitimate context is discarded; chat falls back to persona copy, making answers generic.
- Action: expose the threshold via config per bot and fallback to a simple `slice(0, topK)` when everything is filtered.

### 5. Missing graceful degradation (Medium)
- Evidence: `processAnswerQuery` awaits `retrieveRelevantChunks` without guard (`services/chatbotService.js:136`); any Mongo/Atlas issue throws and the API returns 500.
- Impact: transient Atlas downtime or misconfiguration bricks the chatbot.
- Action: wrap retrieval in try/catch, log the failure, and proceed with history-only context so users still get a response while ops fix search.

### 6. Prompt context pruning (Medium)
- Fix: `generateAnswer` now keeps high-signal short snippets and surfaces a `KB_CONTEXT_AVAILABLE` flag instead of injecting placeholder text.
- Impact: concise answers (e.g. "Yes." or SKU codes) now flow through; prompts stay clean when the knowledge base is empty.
- Action: downstream prompt builders should rely on the boolean flag rather than inserting fallback prose.

### 7. Cache invalidation gap (Medium)

### 7. Cache invalidation gap (Medium)
- Evidence: query results are cached for 120s (`services/queryService.js:12-23`), yet ingestion never clears the corresponding Redis keys.
- Impact: newly uploaded knowledge remains invisible for up to two minutes, confusing operators validating imports.
- Action: after a successful upload, delete cached keys for that chatbot (e.g. pattern `vs:<bot>:*`) or version the cache key with a KB revision counter.

### 8. Duplicate chunk storage (Low)
- Evidence: ingestion always inserts the mapped array (`services/contextService.js:22-30`) with no dedupe check.
- Impact: re-running ingests multiplies identical chunks, inflating storage and diluting relevance.
- Action: hash each chunk (content + chatbot) and skip insert when the hash already exists; alternatively, use `updateOne` with `upsert`.

### Additional suggestions
- Log score distributions during retrieval to calibrate thresholds automatically per bot.
- Consider storing chunk metadata (file name, section heading) alongside `content` to enrich prompts without relying solely on raw text.
